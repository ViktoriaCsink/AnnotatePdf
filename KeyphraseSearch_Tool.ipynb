{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "invisible-phenomenon",
   "metadata": {},
   "source": [
    "## This script processes documents, segments them into pages, calculates the occurrences of certain user-defined keywords, highlights these words with different colours and annotates the document with the page numbers where the keyphrases occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import docx2txt\n",
    "import re\n",
    "import pdfplumber\n",
    "import textract\n",
    "import ocrmypdf\n",
    "import pluggy\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "from decimal import *\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import textract\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import ocrmypdf\n",
    "import pluggy\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "from docx2pdf import convert\n",
    "import pdfkit\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader\n",
    "import fitz\n",
    "import stamper\n",
    "from PDFNetPython3 import *\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import codecs\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import datefinder\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from textwrap import wrap\n",
    "from itertools import chain\n",
    "\n",
    "#import custom functions\n",
    "from segment_pdf import segment_pdf\n",
    "from segment_word import segment_word\n",
    "from segment_html import segment_html\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/Users/Viktoria/Desktop/NLP_AnnotatePdf'\n",
    "data_repo = os.path.join(base, 'Raw_data')\n",
    "\n",
    "#scraper = os.path.join(base, 'Web_scraping')\n",
    "#main = base\n",
    "#results = os.path.join(base, 'Results')\n",
    "#library = os.path.join(base, 'Document_library')\n",
    "#models = os.path.join(base, 'Models')\n",
    "os.chdir(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/jbarlow83/OCRmyPDF.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-mustang",
   "metadata": {},
   "source": [
    "### Step 1. Access the raw data ('Source_List.csv'). This is a list of titles and URLs to be scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a messy table where text data and URLs are mixed. Access everything and bring them to a clean format.\n",
    "os.chdir(data_repo)\n",
    "\n",
    "df = pd.read_csv('Source_List.csv')\n",
    "\n",
    "print('N = ', len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a tidy title. Special characters in the filename will break the download later.\n",
    "\n",
    "def prettify_title(title):\n",
    "    \n",
    "    #Create a nice and tidy title. Special characters in the title will throw errors.\n",
    "    name = re.sub(r'\\W+', ' ', title)\n",
    "    name = re.sub(r'.pdf', '', name)\n",
    "    name = re.sub('^\\s*', '', name)\n",
    "    name = re.sub('\\s*$', '', name)\n",
    "    name = re.sub('[\\[\\]:?!-/\\+=&)(\\\"\\'\\*,]', '', name)\n",
    "    name = name.strip()\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Document']=df['Document'].apply(prettify_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-worship",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
