{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incredible-textbook",
   "metadata": {},
   "source": [
    "## This script processes documents, segments them into pages, calculates the occurrences of certain user-defined keywords, highlights these words with different colours and annotates the document with the page numbers where the keyphrases occur. <br> <br> Viktoria, June 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "auburn-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This script involves convertion between file types. In order to enable this, give jupyter notebook complete disk access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dominican-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import docx2txt\n",
    "import re\n",
    "import pdfplumber\n",
    "import textract\n",
    "import ocrmypdf\n",
    "import pluggy\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "from decimal import *\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import textract\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import ocrmypdf\n",
    "import pluggy\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "from docx2pdf import convert\n",
    "import pdfkit\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader\n",
    "import fitz\n",
    "import stamper\n",
    "from PDFNetPython3 import *\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import codecs\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import datefinder\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from textwrap import wrap\n",
    "from itertools import chain\n",
    "\n",
    "#import custom functions\n",
    "from segment_pdf import segment_pdf\n",
    "from segment_word import segment_word\n",
    "from segment_html import segment_html\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thick-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/Users/Viktoria/Desktop/NLP_AnnotatePdf'\n",
    "raw = os.path.join(base, 'Raw_data')\n",
    "preproc = os.path.join(base, 'Preprocessed_data')\n",
    "keyphraselists = os.path.join(base, 'Keyphrases')\n",
    "results = os.path.join(base, 'Results')\n",
    "\n",
    "os.chdir(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "superb-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "academic-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/jbarlow83/OCRmyPDF.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-faith",
   "metadata": {},
   "source": [
    "### Step 1. Access the raw data ('Source_List.csv'). This is a list of titles and URLs to be scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "specified-outline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  485\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Document</th>\n",
       "      <th>URL</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia AUSTRAC</td>\n",
       "      <td>Money laundering/terrorism financing risk asse...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>30-Mar-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia AUSTRAC</td>\n",
       "      <td>Fintel Alliance</td>\n",
       "      <td></td>\n",
       "      <td>Fintel Alliance is an AUSTRAC initiative estab...</td>\n",
       "      <td>11-May-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia AUSTRAC</td>\n",
       "      <td>SMR case study examples</td>\n",
       "      <td></td>\n",
       "      <td>These case study examples demonstrate the impo...</td>\n",
       "      <td>12-May-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australia AUSTRAC</td>\n",
       "      <td>Coronavirus (COVID-19) – Working with our repo...</td>\n",
       "      <td></td>\n",
       "      <td>Coronavirus update  The global pandemic...</td>\n",
       "      <td>07-Apr-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australia AUSTRAC</td>\n",
       "      <td>Junket tour operations in Australia risk asses...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10-May-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Source                                           Document URL  \\\n",
       "0  Australia AUSTRAC  Money laundering/terrorism financing risk asse...       \n",
       "1  Australia AUSTRAC                                    Fintel Alliance       \n",
       "2  Australia AUSTRAC                            SMR case study examples       \n",
       "3  Australia AUSTRAC  Coronavirus (COVID-19) – Working with our repo...       \n",
       "4  Australia AUSTRAC  Junket tour operations in Australia risk asses...       \n",
       "\n",
       "                                                Text       Date  \n",
       "0                                                     30-Mar-21  \n",
       "1  Fintel Alliance is an AUSTRAC initiative estab...  11-May-21  \n",
       "2  These case study examples demonstrate the impo...  12-May-21  \n",
       "3         Coronavirus update  The global pandemic...  07-Apr-21  \n",
       "4                                                     10-May-21  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is a messy table where text data and URLs are mixed. Access everything and bring them to a clean format.\n",
    "os.chdir(raw)\n",
    "\n",
    "df = pd.read_csv('Source_List.csv')\n",
    "\n",
    "print('N = ', len(df))\n",
    "df = df.fillna('')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interracial-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a tidy title. Special characters in the filename will break the download later.\n",
    "\n",
    "def prettify_title(title):\n",
    "    \n",
    "    #Create a nice and tidy title. Special characters in the title will throw errors.\n",
    "    name = re.sub(r'\\W+', ' ', title)\n",
    "    name = re.sub(r'.pdf', '', name)\n",
    "    name = re.sub('^\\s*', '', name)\n",
    "    name = re.sub('\\s*$', '', name)\n",
    "    name = re.sub('[\\[\\]:?!-/\\+=&)(\\\"\\'\\*,]', '', name)\n",
    "    name = name.strip()\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "equivalent-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Document']=df['Document'].apply(prettify_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-tennessee",
   "metadata": {},
   "source": [
    "### Step 2. Visit the URLs and get the text. <br> Everything is converted into pdf and broken down into pages, because the density of keyphrases will be calculated on each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minute-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the segmenting functions for pdf, word and html. Get the text directly if it is already in the dataframe.\n",
    "#Change download=0 if you don't want the documents to arrive into the pre-processed folder.\n",
    "\n",
    "def process_document(title, url, text):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #If the text column is empty, access the URL\n",
    "        if not text:\n",
    "            \n",
    "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'})\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            #word\n",
    "            if '.docx' in url:\n",
    "                \n",
    "                content = segment_word(title, response, 1)\n",
    "\n",
    "            #pdf \n",
    "            elif '.pdf' in url or 'PDF' in soup.text[0:50]:\n",
    "\n",
    "                content = segment_pdf(title, response, 1)\n",
    "                \n",
    "\n",
    "            #probably html\n",
    "            else:\n",
    "                content = segment_html(title, url, 1)\n",
    "\n",
    "\n",
    "        #text is already in the dataframe: just write it out as pdf\n",
    "        elif text:\n",
    "\n",
    "            #download as pdf\n",
    "            document = pdfkit.from_string(text, title + '.pdf')\n",
    "\n",
    "            #read in the text as pages\n",
    "            content = {}\n",
    "\n",
    "            with pdfplumber.open(document) as pdf:\n",
    "\n",
    "                pages = pdf.pages\n",
    "                for count,page in enumerate(pages):\n",
    "                    content[count+1] = page.extract_text()\n",
    "                    \n",
    "    except:\n",
    "        content = 'download error'\n",
    "        print(title, ' ', url)\n",
    "    \n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now access the urls and get the text all in one go.\n",
    "os.chdir(preproc)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df['TextResult'] = [m for m in map(process_document, df.Document, tqdm(df.URL), df.Text)]\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.TextResult!='download error']\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "handed-vaccine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Document</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>TextResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>Global safe assets</td>\n",
       "      <td>https://www.bis.org/publ/work399.pdf</td>\n",
       "      <td>18-Dec-12</td>\n",
       "      <td>{1: ' \n",
       " \n",
       "   \n",
       "BIS Working Papers \n",
       "No 399 \n",
       " \n",
       "Glo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>Crises and rescues liquidity transmission thro...</td>\n",
       "      <td>https://www.bis.org/publ/work576.pdf</td>\n",
       "      <td>26-Aug-16</td>\n",
       "      <td>{1: ' \n",
       " \n",
       "    BIS Working Papers\n",
       "No 576 \n",
       "  Cris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>German unification and the demand for German M3</td>\n",
       "      <td>https://www.bis.org/publ/work21.pdf</td>\n",
       "      <td>20-Sep-94</td>\n",
       "      <td>{1: 'BIS \n",
       "  \n",
       "Working  paper  No.  21 \n",
       "  \n",
       " \n",
       "GER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>The housing meltdown Why did it happen in the ...</td>\n",
       "      <td>https://www.bis.org/publ/work259.pdf</td>\n",
       "      <td>18-Sep-08</td>\n",
       "      <td>{1: ' \n",
       " \n",
       "   \n",
       "BIS Working Papers \n",
       "No 259 \n",
       " \n",
       "The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>Big data and machine learning in central banking</td>\n",
       "      <td>https://www.bis.org/publ/work930.pdf</td>\n",
       "      <td>04-Mar-21</td>\n",
       "      <td>{1: ' \n",
       " \n",
       "    BIS Working Papers \n",
       "No 930 \n",
       "  Big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>Recent RMB policy and currency co movements</td>\n",
       "      <td>https://www.bis.org/publ/work727.pdf</td>\n",
       "      <td>11-Jun-18</td>\n",
       "      <td>{1: ' \n",
       " \n",
       "    BIS Working Papers \n",
       "No 727 \n",
       "  Rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>Exchange rate risk and local currency sovereig...</td>\n",
       "      <td>https://www.bis.org/publ/work474.pdf</td>\n",
       "      <td>18-Dec-14</td>\n",
       "      <td>{1: ' \n",
       " \n",
       "    BIS Working Papers\n",
       "No 474 \n",
       "  Exch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>The pricing of portfolio credit risk</td>\n",
       "      <td>https://www.bis.org/publ/work214.pdf</td>\n",
       "      <td>15-Sep-06</td>\n",
       "      <td>{1: ' \n",
       " \n",
       "   \n",
       "BIS Working Papers \n",
       "No 214 \n",
       " \n",
       "The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>The response of short term bank lending rates ...</td>\n",
       "      <td>https://www.bis.org/publ/work27.pdf</td>\n",
       "      <td>21-May-95</td>\n",
       "      <td>{1: 'BIS \n",
       "  \n",
       "Working  paper  No.  27 \n",
       "  \n",
       "THE  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>Trade linkages and the globalisation of inflat...</td>\n",
       "      <td>https://www.bis.org/publ/work447.pdf</td>\n",
       "      <td>14-Apr-14</td>\n",
       "      <td>{1: ' \n",
       " \n",
       "    BIS Working Papers\n",
       "No 447 \n",
       "  Trad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>All the money in europe An investigation of th...</td>\n",
       "      <td>https://www.bis.org/publ/work19.pdf</td>\n",
       "      <td>19-Oct-93</td>\n",
       "      <td>{1: 'BIS \n",
       "  \n",
       "Working  paper  No.  19 \n",
       "     \n",
       "AL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>The international dimensions of macroprudentia...</td>\n",
       "      <td>https://www.bis.org/publ/work643.pdf</td>\n",
       "      <td>08-Jun-17</td>\n",
       "      <td>{1: ' \n",
       " \n",
       "    BIS Working Papers\n",
       "No 643 \n",
       "  The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>Inflation expectations anchoring new insights ...</td>\n",
       "      <td>https://www.bis.org/publ/work809.pdf</td>\n",
       "      <td>04-Sep-19</td>\n",
       "      <td>{1: ' \n",
       " \n",
       "    BIS Working Papers \n",
       "No 809 \n",
       "  Inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>Measuring bank competition in China a comparis...</td>\n",
       "      <td>https://www.bis.org/publ/work422.pdf</td>\n",
       "      <td>21-Aug-13</td>\n",
       "      <td>{1: ' \n",
       " \n",
       "   \n",
       "BIS Working Papers\n",
       "No 422 \n",
       " \n",
       "Meas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>The impact of macroprudential policies and the...</td>\n",
       "      <td>https://www.bis.org/publ/work636.pdf</td>\n",
       "      <td>03-May-17</td>\n",
       "      <td>{1: 'BIS Working Papers\n",
       "No 636 \n",
       "The impact of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>The supply side of household finance</td>\n",
       "      <td>https://www.bis.org/publ/work531.pdf</td>\n",
       "      <td>03-Dec-15</td>\n",
       "      <td>{1: ' \n",
       " \n",
       "    BIS Working Papers\n",
       "No 531 \n",
       "  The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>Do small bank deposits run more than large one...</td>\n",
       "      <td>https://www.bis.org/publ/work724.pdf</td>\n",
       "      <td>16-May-18</td>\n",
       "      <td>{1: ' \n",
       " \n",
       "    BIS Working Papers \n",
       "No 724 \n",
       "  Do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>Can liquidity risk be subsumed in credit risk ...</td>\n",
       "      <td>https://www.bis.org/publ/work101.pdf</td>\n",
       "      <td>02-Jul-01</td>\n",
       "      <td>{1: 'BIS Working Papers\n",
       "No 101\n",
       "Can liquidity r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>The international propagation of the financial...</td>\n",
       "      <td>https://www.bis.org/publ/work348.pdf</td>\n",
       "      <td>05-Jul-11</td>\n",
       "      <td>{1: ' \n",
       " \n",
       "   \n",
       "BIS Working Papers \n",
       "No 348 \n",
       " \n",
       "The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>To err is human rating agencies and the interw...</td>\n",
       "      <td>https://www.bis.org/publ/work335.pdf</td>\n",
       "      <td>30-Dec-10</td>\n",
       "      <td>{1: ' \n",
       "  \n",
       "BIS Working Papers \n",
       "No 335 \n",
       " \n",
       "To err...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Source                                           Document  \\\n",
       "0   BIS Working Papers                                 Global safe assets   \n",
       "1   BIS Working Papers  Crises and rescues liquidity transmission thro...   \n",
       "2   BIS Working Papers    German unification and the demand for German M3   \n",
       "3   BIS Working Papers  The housing meltdown Why did it happen in the ...   \n",
       "4   BIS Working Papers   Big data and machine learning in central banking   \n",
       "5   BIS Working Papers        Recent RMB policy and currency co movements   \n",
       "6   BIS Working Papers  Exchange rate risk and local currency sovereig...   \n",
       "7   BIS Working Papers               The pricing of portfolio credit risk   \n",
       "8   BIS Working Papers  The response of short term bank lending rates ...   \n",
       "9   BIS Working Papers  Trade linkages and the globalisation of inflat...   \n",
       "10  BIS Working Papers  All the money in europe An investigation of th...   \n",
       "11  BIS Working Papers  The international dimensions of macroprudentia...   \n",
       "12  BIS Working Papers  Inflation expectations anchoring new insights ...   \n",
       "13  BIS Working Papers  Measuring bank competition in China a comparis...   \n",
       "14  BIS Working Papers  The impact of macroprudential policies and the...   \n",
       "15  BIS Working Papers               The supply side of household finance   \n",
       "16  BIS Working Papers  Do small bank deposits run more than large one...   \n",
       "17  BIS Working Papers  Can liquidity risk be subsumed in credit risk ...   \n",
       "18  BIS Working Papers  The international propagation of the financial...   \n",
       "19  BIS Working Papers  To err is human rating agencies and the interw...   \n",
       "\n",
       "                                     URL       Date  \\\n",
       "0   https://www.bis.org/publ/work399.pdf  18-Dec-12   \n",
       "1   https://www.bis.org/publ/work576.pdf  26-Aug-16   \n",
       "2    https://www.bis.org/publ/work21.pdf  20-Sep-94   \n",
       "3   https://www.bis.org/publ/work259.pdf  18-Sep-08   \n",
       "4   https://www.bis.org/publ/work930.pdf  04-Mar-21   \n",
       "5   https://www.bis.org/publ/work727.pdf  11-Jun-18   \n",
       "6   https://www.bis.org/publ/work474.pdf  18-Dec-14   \n",
       "7   https://www.bis.org/publ/work214.pdf  15-Sep-06   \n",
       "8    https://www.bis.org/publ/work27.pdf  21-May-95   \n",
       "9   https://www.bis.org/publ/work447.pdf  14-Apr-14   \n",
       "10   https://www.bis.org/publ/work19.pdf  19-Oct-93   \n",
       "11  https://www.bis.org/publ/work643.pdf  08-Jun-17   \n",
       "12  https://www.bis.org/publ/work809.pdf  04-Sep-19   \n",
       "13  https://www.bis.org/publ/work422.pdf  21-Aug-13   \n",
       "14  https://www.bis.org/publ/work636.pdf  03-May-17   \n",
       "15  https://www.bis.org/publ/work531.pdf  03-Dec-15   \n",
       "16  https://www.bis.org/publ/work724.pdf  16-May-18   \n",
       "17  https://www.bis.org/publ/work101.pdf  02-Jul-01   \n",
       "18  https://www.bis.org/publ/work348.pdf  05-Jul-11   \n",
       "19  https://www.bis.org/publ/work335.pdf  30-Dec-10   \n",
       "\n",
       "                                           TextResult  \n",
       "0   {1: ' \n",
       " \n",
       "   \n",
       "BIS Working Papers \n",
       "No 399 \n",
       " \n",
       "Glo...  \n",
       "1   {1: ' \n",
       " \n",
       "    BIS Working Papers\n",
       "No 576 \n",
       "  Cris...  \n",
       "2   {1: 'BIS \n",
       "  \n",
       "Working  paper  No.  21 \n",
       "  \n",
       " \n",
       "GER...  \n",
       "3   {1: ' \n",
       " \n",
       "   \n",
       "BIS Working Papers \n",
       "No 259 \n",
       " \n",
       "The...  \n",
       "4   {1: ' \n",
       " \n",
       "    BIS Working Papers \n",
       "No 930 \n",
       "  Big...  \n",
       "5   {1: ' \n",
       " \n",
       "    BIS Working Papers \n",
       "No 727 \n",
       "  Rec...  \n",
       "6   {1: ' \n",
       " \n",
       "    BIS Working Papers\n",
       "No 474 \n",
       "  Exch...  \n",
       "7   {1: ' \n",
       " \n",
       "   \n",
       "BIS Working Papers \n",
       "No 214 \n",
       " \n",
       "The...  \n",
       "8   {1: 'BIS \n",
       "  \n",
       "Working  paper  No.  27 \n",
       "  \n",
       "THE  ...  \n",
       "9   {1: ' \n",
       " \n",
       "    BIS Working Papers\n",
       "No 447 \n",
       "  Trad...  \n",
       "10  {1: 'BIS \n",
       "  \n",
       "Working  paper  No.  19 \n",
       "     \n",
       "AL...  \n",
       "11  {1: ' \n",
       " \n",
       "    BIS Working Papers\n",
       "No 643 \n",
       "  The ...  \n",
       "12  {1: ' \n",
       " \n",
       "    BIS Working Papers \n",
       "No 809 \n",
       "  Inf...  \n",
       "13  {1: ' \n",
       " \n",
       "   \n",
       "BIS Working Papers\n",
       "No 422 \n",
       " \n",
       "Meas...  \n",
       "14  {1: 'BIS Working Papers\n",
       "No 636 \n",
       "The impact of ...  \n",
       "15  {1: ' \n",
       " \n",
       "    BIS Working Papers\n",
       "No 531 \n",
       "  The ...  \n",
       "16  {1: ' \n",
       " \n",
       "    BIS Working Papers \n",
       "No 724 \n",
       "  Do ...  \n",
       "17  {1: 'BIS Working Papers\n",
       "No 101\n",
       "Can liquidity r...  \n",
       "18  {1: ' \n",
       " \n",
       "   \n",
       "BIS Working Papers \n",
       "No 348 \n",
       " \n",
       "The...  \n",
       "19  {1: ' \n",
       "  \n",
       "BIS Working Papers \n",
       "No 335 \n",
       " \n",
       "To err...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(base)\n",
    "df = pd.read_csv('Keyphrases_input.csv')\n",
    "df = df.drop(df.columns.difference(['Source', 'Document', 'URL', 'TextResult', 'Date']), axis=1)\n",
    "df = df[df.TextResult!='download error']\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#make sure cleaned_text is an orderly list of strings\n",
    "if type(df.loc[0, 'TextResult']) == str:\n",
    "     df['TextResult'] = [eval(df.loc[i, 'TextResult']) for i,v in df.iterrows()]\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-sunday",
   "metadata": {},
   "source": [
    "### Step 3. Import the keyphrases that we are interested in for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spoken-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import keywords. Some keywords contain other keywords. Order them by length, because the longer phrases will be searched for first.\n",
    "\n",
    "os.chdir(keyphraselists)\n",
    "\n",
    "with open('topic1.txt', 'r+') as f:\n",
    "    wordlist = f.readlines()  \n",
    "    wordlist = [re.sub('\\n', '', w) for w in wordlist]\n",
    "    wordlist = [w.lower() for w in wordlist]\n",
    "    t1 = sorted(wordlist, key=len, reverse=True)\n",
    "    del f, wordlist\n",
    "    \n",
    "with open('topic2.txt', 'r+') as f:\n",
    "    wordlist = f.readlines()  \n",
    "    wordlist = [re.sub('\\n', '', w) for w in wordlist]\n",
    "    wordlist = [w.lower() for w in wordlist]\n",
    "    t2 = sorted(wordlist, key=len, reverse=True)\n",
    "    del f, wordlist\n",
    "    \n",
    "with open('topic3.txt', 'r+') as f:\n",
    "    wordlist = f.readlines()  \n",
    "    wordlist = [re.sub('\\n', '', w) for w in wordlist]\n",
    "    wordlist = [w.lower() for w in wordlist]\n",
    "    t3 = sorted(wordlist, key=len, reverse=True)\n",
    "    del f, wordlist\n",
    "    \n",
    "with open('topic4.txt', 'r+') as f:\n",
    "    wordlist = f.readlines()  \n",
    "    wordlist = [re.sub('\\n', '', w) for w in wordlist]\n",
    "    wordlist = [w.lower() for w in wordlist]\n",
    "    t4 = sorted(wordlist, key=len, reverse=True)\n",
    "    del f, wordlist\n",
    "    \n",
    "with open('topic5.txt', 'r+') as f:\n",
    "    wordlist = f.readlines()  \n",
    "    wordlist = [re.sub('\\n', '', w) for w in wordlist]\n",
    "    wordlist = [w.lower() for w in wordlist]\n",
    "    t5 = sorted(wordlist, key=len, reverse=True) #longer words before shorter words\n",
    "    del f, wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-dubai",
   "metadata": {},
   "source": [
    "### Step 4. Apply some basic text cleaning. <br> The proportion of keyphrases/page will be calculated, so don't count stopwords, artefacts etc into the total number of words per page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "welsh-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All non-English words will be disposed of. Keep important non-English words on this list.\n",
    "os.chdir(base)\n",
    "\n",
    "result = docx2txt.process(\"informative_words.docx\")\n",
    "informative = re.findall(r'\\w+', result)\n",
    "informative = [i.lower() for i in informative]\n",
    "\n",
    "en_words = set(nltk.corpus.words.words())\n",
    "stop_words = [s for s in stopwords.words('english') if s not in informative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "running-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    \n",
    "    cleaned_text = {}\n",
    "    \n",
    "    if isinstance(text, dict):\n",
    "    \n",
    "        for k,v in text.items():\n",
    "            \n",
    "            if isinstance(v, str):\n",
    "                v = re.sub(r'[\\t]', ' ', v)\n",
    "                v = re.sub(r'[\\n]', ' ', v)\n",
    "                v = re.sub('[^a-zA-Z0-9 ]', ' ', v)\n",
    "                v = ' '.join([word for word in v.split() if word in en_words and word not in stop_words])\n",
    "                cleaned_text[k] = v\n",
    "                \n",
    "                \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "small-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TextResult'] = df['TextResult'].apply(basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "respective-pendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Document</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>TextResult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>Global safe assets</td>\n",
       "      <td>https://www.bis.org/publ/work399.pdf</td>\n",
       "      <td>18-Dec-12</td>\n",
       "      <td>{1: 'No safe assets Pierre Olivier Olivier Jea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>Crises and rescues liquidity transmission thro...</td>\n",
       "      <td>https://www.bis.org/publ/work576.pdf</td>\n",
       "      <td>26-Aug-16</td>\n",
       "      <td>{1: 'No liquidity transmission international C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>German unification and the demand for German M3</td>\n",
       "      <td>https://www.bis.org/publ/work21.pdf</td>\n",
       "      <td>20-Sep-94</td>\n",
       "      <td>{1: 'paper No Stefan September', 3: 'Stefan Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>The housing meltdown Why did it happen in the ...</td>\n",
       "      <td>https://www.bis.org/publ/work259.pdf</td>\n",
       "      <td>18-Sep-08</td>\n",
       "      <td>{1: 'No The housing happen September classific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BIS Working Papers</td>\n",
       "      <td>Big data and machine learning in central banking</td>\n",
       "      <td>https://www.bis.org/publ/work930.pdf</td>\n",
       "      <td>04-Mar-21</td>\n",
       "      <td>{1: 'No data machine learning central banking ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Source                                           Document  \\\n",
       "0  BIS Working Papers                                 Global safe assets   \n",
       "1  BIS Working Papers  Crises and rescues liquidity transmission thro...   \n",
       "2  BIS Working Papers    German unification and the demand for German M3   \n",
       "3  BIS Working Papers  The housing meltdown Why did it happen in the ...   \n",
       "4  BIS Working Papers   Big data and machine learning in central banking   \n",
       "\n",
       "                                    URL       Date  \\\n",
       "0  https://www.bis.org/publ/work399.pdf  18-Dec-12   \n",
       "1  https://www.bis.org/publ/work576.pdf  26-Aug-16   \n",
       "2   https://www.bis.org/publ/work21.pdf  20-Sep-94   \n",
       "3  https://www.bis.org/publ/work259.pdf  18-Sep-08   \n",
       "4  https://www.bis.org/publ/work930.pdf  04-Mar-21   \n",
       "\n",
       "                                          TextResult  \n",
       "0  {1: 'No safe assets Pierre Olivier Olivier Jea...  \n",
       "1  {1: 'No liquidity transmission international C...  \n",
       "2  {1: 'paper No Stefan September', 3: 'Stefan Se...  \n",
       "3  {1: 'No The housing happen September classific...  \n",
       "4  {1: 'No data machine learning central banking ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.TextResult!={}]\n",
    "df = df.reset_index(drop=True)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-tourist",
   "metadata": {},
   "source": [
    "### Step 5. Calculate the percentage of key phrases in each topic on each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sophisticated-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give more meaningful names to the topics. Here the names have been changed to conceal the purpose of the project.\n",
    "\n",
    "topics = ['Topic1', 'Topic2', 'Topic3', 'Topic4', 'Topic5']\n",
    "phrases = [t1, t2, t3, t4, t5]\n",
    "\n",
    "for topic,phraselist in zip(topics, phrases):\n",
    "\n",
    "    #the proportion of keyphrases on each page is saved here as an np array\n",
    "    df[topic] = ''\n",
    "\n",
    "    for r,v in df.iterrows():\n",
    "\n",
    "        #each text\n",
    "        doc = df.loc[r, 'TextResult']\n",
    "\n",
    "        #number of hits on a page\n",
    "        numsOnPage = [0] #Page 0 will not have anything\n",
    "        \n",
    "        #total number of words on a page\n",
    "        lenPages = [0.001] #avoid division by zero\n",
    "\n",
    "        #each page\n",
    "        for k,v in doc.items():\n",
    "\n",
    "            num = 0\n",
    "            len_page = len(v.split()) #number of words on a page\n",
    "\n",
    "            if len_page >= 50: #not an empty page\n",
    "\n",
    "                #count the phrases in that topic\n",
    "                for p in phraselist:\n",
    "\n",
    "                    #length of the phrase\n",
    "                    len_phrase = len(p.split())\n",
    "\n",
    "                    #number of times the phrase appears (corrected for the length of the phrase)\n",
    "                    target = ' '+p+' '  #only count the occurrence of full words\n",
    "                    hits = v.lower().count(target)\n",
    "                    v = v.replace(p, '') #pop the phrase we have already found\n",
    "                    num = num + hits*len_phrase\n",
    "\n",
    "            else:\n",
    "                len_page = 0.001 \n",
    "\n",
    "            numsOnPage.append(num)\n",
    "            lenPages.append(len_page)\n",
    "            \n",
    "        #proportion of keyphrases on each page\n",
    "        props = np.divide(numsOnPage, lenPages)\n",
    "        \n",
    "        \n",
    "        if max(props) >= 0.05:\n",
    "            df[topic][r] = props #list with the prop phrases on each page\n",
    "        else:\n",
    "            df[topic][r] = ''\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "included-liberal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of relevant documents:  115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-e49862f1315f>:8: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if df.loc[r,t] != '':\n"
     ]
    }
   ],
   "source": [
    "#How many documents have been found?\n",
    "\n",
    "topics = ['Topic1', 'Topic2', 'Topic3', 'Topic4', 'Topic5']\n",
    "\n",
    "vals=[]\n",
    "for r,v in df.iterrows():\n",
    "    for t in topics:\n",
    "        if df.loc[r,t] != '':\n",
    "            vals.append(r)\n",
    "            \n",
    "print('Total number of relevant documents: ', len(set(vals)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-angle",
   "metadata": {},
   "source": [
    "### Step 6. Highlight the keyphrases in the text so that the reader can get a sense of the topic density <br> Each topic will be highlighted with a different colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stuck-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a dictinary with the colours\n",
    "\n",
    "colourdict = {'Topic1': (0.866, 0.627, 0.866), 'Topic2': (1, 0.27, 0), 'Topic3': (0, 0.75, 1), 'Topic4': (0.56, 0.933, 0.56), 'Topic5': (1, 0.843, 0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-africa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [01:56,  1.44s/it]"
     ]
    }
   ],
   "source": [
    "topics = ['Topic1', 'Topic2', 'Topic3', 'Topic4', 'Topic5']\n",
    "phrases = [t1, t2, t3, t4, t5]\n",
    "\n",
    "for r,v in tqdm(df.iterrows()):\n",
    "    \n",
    "    os.chdir(preproc)\n",
    "    \n",
    "    if r in vals:\n",
    "\n",
    "        name = df.loc[r, 'Document'] + '.pdf'\n",
    "        doc = fitz.open(name)\n",
    "\n",
    "        for page in doc:\n",
    "            ### SEARCH\n",
    "\n",
    "            for topic,phraselist in zip(topics,phrases):\n",
    "\n",
    "                for p in phraselist: \n",
    "                    text = ' '+p+' '\n",
    "                    text_instances = page.searchFor(text)\n",
    "\n",
    "                    ### HIGHLIGHT\n",
    "                    for inst in text_instances:\n",
    "                        highlight = page.addHighlightAnnot(inst)\n",
    "                        highlight.setColors({\"stroke\": colourdict.get(topic)})\n",
    "                        highlight.update()\n",
    "\n",
    "\n",
    "        os.chdir(os.path.join(results))\n",
    "\n",
    "        ### OUTPUT\n",
    "        doc.save(name, incremental=True, encryption=fitz.PDF_ENCRYPT_KEEP) #same doc\n",
    "        #doc.save('Rename '+name, garbage=4, deflate=True, clean=True) #new doc\n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "        #Note that incremental save will only work if there have been changes made to the document. \n",
    "        #If the highlights are not found, the document will not be moved to the results folder. \n",
    "        #That is fine, because the document is likely to be an artefact.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-joseph",
   "metadata": {},
   "source": [
    "### Step 7. Add a legend box to the first page of the documents to show which topic is highlighted with which colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDFNet.Initialize()\n",
    "\n",
    "os.chdir(os.path.join(results, 'Keyphrases'))\n",
    "\n",
    "texts = os.listdir(os.path.join(results, 'Keyphrases'))\n",
    "\n",
    "for text in tqdm(texts): \n",
    "\n",
    "    doc = PDFDoc(text)\n",
    "    page = doc.GetPage(1)\n",
    "    \n",
    "    # Plum\n",
    "    txtannot = FreeText.Create(doc.GetSDFDoc(), Rect(20, 760, 120, 770))\n",
    "    txtannot.SetContentRect(Rect(20, 760, 120, 770))\n",
    "    txtannot.SetContents( \"Topic 1\")\n",
    "    txtannot.SetBorderStyle( BorderStyle( BorderStyle.e_solid, 0, 10, 20 ), True )\n",
    "    txtannot.SetEndingStyle(LineAnnot.e_ClosedArrow )\n",
    "    txtannot.SetColor(ColorPt( 1, 1, 1) )\n",
    "    txtannot.SetLineColor(ColorPt(0.866, 0.627, 0.866), 3)\n",
    "    txtannot.SetFontSize(8)\n",
    "    txtannot.SetQuaddingFormat(1)\n",
    "    page.AnnotPushBack(txtannot)\n",
    "    txtannot.RefreshAppearance()\n",
    "    \n",
    "    # Orange red\n",
    "    txtannot = FreeText.Create(doc.GetSDFDoc(), Rect(20, 780, 120, 790))\n",
    "    txtannot.SetContentRect(Rect(20, 780, 120, 790))\n",
    "    txtannot.SetContents( \"Topic 2\")\n",
    "    txtannot.SetBorderStyle( BorderStyle( BorderStyle.e_solid, 0, 10, 20 ), True )\n",
    "    txtannot.SetEndingStyle(LineAnnot.e_ClosedArrow )\n",
    "    txtannot.SetColor(ColorPt( 1, 1, 1) )\n",
    "    txtannot.SetLineColor(ColorPt(1, 0.27, 0), 3)\n",
    "    txtannot.SetFontSize(8)\n",
    "    txtannot.SetQuaddingFormat(1)\n",
    "    page.AnnotPushBack(txtannot)\n",
    "    txtannot.RefreshAppearance()\n",
    "    \n",
    "    # Deep skye blue\n",
    "    txtannot = FreeText.Create(doc.GetSDFDoc(), Rect(20, 770, 120, 780))\n",
    "    txtannot.SetContentRect(Rect(20, 770, 120, 780))\n",
    "    txtannot.SetContents( \"Topic 3\")\n",
    "    txtannot.SetBorderStyle( BorderStyle( BorderStyle.e_solid, 0, 10, 20 ), True )\n",
    "    txtannot.SetEndingStyle(LineAnnot.e_ClosedArrow )\n",
    "    txtannot.SetColor(ColorPt( 1, 1, 1) )\n",
    "    txtannot.SetLineColor(ColorPt(0, 0.75, 1), 3)\n",
    "    txtannot.SetFontSize(8)\n",
    "    txtannot.SetQuaddingFormat(1)\n",
    "    page.AnnotPushBack(txtannot)\n",
    "    txtannot.RefreshAppearance()\n",
    "\n",
    "    # Pale green\n",
    "    txtannot = FreeText.Create(doc.GetSDFDoc(), Rect(20, 790, 120, 800))\n",
    "    txtannot.SetContentRect(Rect(20, 790, 120, 800))\n",
    "    txtannot.SetContents( \"Topic 4\")\n",
    "    txtannot.SetBorderStyle( BorderStyle( BorderStyle.e_solid, 0, 10, 20 ), True )\n",
    "    txtannot.SetEndingStyle(LineAnnot.e_ClosedArrow )\n",
    "    txtannot.SetColor(ColorPt( 1, 1, 1) )\n",
    "    txtannot.SetLineColor(ColorPt(0.56, 0.933, 0.56), 3)\n",
    "    txtannot.SetFontSize(8)\n",
    "    txtannot.SetQuaddingFormat(1)\n",
    "    page.AnnotPushBack(txtannot)\n",
    "    txtannot.RefreshAppearance()\n",
    "\n",
    "    # Gold\n",
    "    txtannot = FreeText.Create(doc.GetSDFDoc(), Rect(20, 750, 120, 760))\n",
    "    txtannot.SetContentRect(Rect(20, 750, 120, 760))\n",
    "    txtannot.SetContents( \"Topic 5\")\n",
    "    txtannot.SetBorderStyle( BorderStyle( BorderStyle.e_solid, 0, 10, 20 ), True )\n",
    "    txtannot.SetEndingStyle(LineAnnot.e_ClosedArrow )\n",
    "    txtannot.SetColor(ColorPt( 1, 1, 1) )\n",
    "    txtannot.SetLineColor(ColorPt(1, 0.843, 0), 3)\n",
    "    txtannot.SetFontSize(8)\n",
    "    txtannot.SetQuaddingFormat(1)\n",
    "    page.AnnotPushBack(txtannot)\n",
    "    txtannot.RefreshAppearance()\n",
    "\n",
    "    doc.Save(text, SDFDoc.e_linearized)\n",
    "    doc.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-marking",
   "metadata": {},
   "source": [
    "### Step 8. Annotate the documents with the page numbers where relevant texts appear. <br> i.e. the proportion of keywords is > 5% on the page for a given topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe called annotations where each document is represented with the page numbers for each topic\n",
    "\n",
    "annotations = pd.DataFrame()\n",
    "\n",
    "annotations['Document'] = df['Document']\n",
    "\n",
    "topics = ['Topic1', 'Topic2', 'Topic3', 'Topic4', 'Topic5']\n",
    "phrases = [t1, t2, t3, t4, t5]\n",
    "\n",
    "for topic,phraselist in zip(topics, phrases):\n",
    "\n",
    "    for r,v in annotations.iterrows():\n",
    "\n",
    "        datapoint = df.loc[r, topic]\n",
    "\n",
    "        #if at least 1 page exceeded the threshold on that topic\n",
    "        if datapoint != '':\n",
    "\n",
    "            #Select the page as relevant if the proportion of keyphrases is > x% on that page\n",
    "            pages = []\n",
    "            for pp,prop in enumerate(datapoint):\n",
    "                if prop >= 0.05:\n",
    "                    pages.append(pp)\n",
    "\n",
    "            #Get the page annotations\n",
    "            annot = []\n",
    "            start = pages[0]\n",
    "\n",
    "            for c,p in enumerate(pages):\n",
    "\n",
    "                #only 1 page in total\n",
    "                if len(pages) == 1:\n",
    "                    annot.append(p)\n",
    "                    break\n",
    "\n",
    "                #if this is not the last page\n",
    "                if p != pages[-1]:\n",
    "\n",
    "                    if pages[c] + 1 == pages[c+1]:\n",
    "                        continue\n",
    "                    elif start != p:\n",
    "                        annot.append((start,p))\n",
    "                        start = pages[c+1]\n",
    "                    elif start == p:\n",
    "                        annot.append(p)\n",
    "                        start = pages[c+1]\n",
    "\n",
    "                #last page\n",
    "                else:\n",
    "\n",
    "                    if pages[-1] - 1 == pages[-2]:\n",
    "                        annot.append((start, p))\n",
    "                    else:\n",
    "                        annot.append(p)\n",
    "\n",
    "\n",
    "            tidy_annot = []\n",
    "            for a in annot:\n",
    "                if isinstance(a, tuple):\n",
    "                    tidy_annot.append(''.join('p' + str(a[0]) + '-' + str(a[1])))\n",
    "                else:\n",
    "                    tidy_annot.append(''.join('p'+ str(a)))\n",
    "\n",
    "            pagestring = ', '.join([t for t in tidy_annot])\n",
    "\n",
    "            #add the nice and tidy pagestring into a column\n",
    "            annotations.loc[r, topic] = pagestring\n",
    "            \n",
    "        else:\n",
    "            annotations.loc[r, topic] = ''\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
